# Intro to Data Science
Data science is an interdisciplinary field focused on extracting meaningful insights and knowledge from data. It combines techniques from statistics, computer science, and domain expertise to collect, analyze, and interpret large volumes of information. Through methods like data cleaning, visualization, and statistical modeling, data scientists turn raw data into understandable patterns that inform decision-making. This process often involves working with various types of data, including structured databases, unstructured text, images, or real-time sensor outputs.

A major part of data science involves building predictive models using machine learning algorithms. These models can forecast trends, detect anomalies, or automate complex tasks. Data science is applied across many industries—such as healthcare, finance, marketing, and technology—to solve problems, optimize processes, and innovate new solutions. Ultimately, data science helps organizations understand their data better and use it to make smarter, evidence-based decisions.

## Big data
The data science field ways devised because datasets started becoming larger and extracting information from them has become difficoult. Big data refers to extremely large and complex datasets that are difficult to process using traditional data management tools. It is characterized by high volume, velocity, and variety—meaning data is generated in massive amounts, at high speed, and in many different forms such as text, images, videos, and sensor readings. Big data technologies and analytics allow organizations to store, manage, and analyze this information to uncover patterns, predict trends, and make informed decisions. As industries increasingly rely on digital systems, big data has become essential for improving efficiency, understanding customer behavior, and driving innovation. Data Scientists often work with Big datasets.

### Tools for working with data
Hadoop is an open-source framework that enables the distributed processing of large datasets across clusters of computers.

HDFS (Hadoop Distributed File System) is Hadoop’s storage system that stores data across multiple machines for scalability and fault tolerance.

Hive is a data warehousing tool in the Hadoop ecosystem that allows users to query large datasets using SQL-like commands.

Spark is a fast, in-memory data processing engine designed for large-scale analytics, machine learning, and real-time data processing.

## Data mining
Data mining is the process of discovering useful patterns, relationships, and trends within large datasets using statistical and computational techniques. It helps organizations make better decisions by revealing insights that are not immediately obvious from raw data.

## Machine learning
Machine learning is a key component of data science, providing algorithms that allow computers to identify patterns and make predictions from large datasets. In data science, machine learning helps automate analysis, improve accuracy, and uncover insights that would be difficult to detect through traditional methods.

## Data storage types
A data warehouse is a centralized system that stores large volumes of structured data from multiple sources to support reporting, analysis, and decision-making.

    Data Marts are focused subsets of a data warehouse designed to serve the specific needs of a particular team or department.

    Data Lakes are large repositories that store raw, unstructured, and structured data in its native format for flexible analysis.

    ETL (Extract, Transform, Load) is a process that collects data from various sources, cleans and reshapes it, and loads it into a target system like a data warehouse.

    Data Pipelines are automated workflows that move data between systems, ensuring it is collected, processed, and delivered where it is needed.
